{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2bac3b",
   "metadata": {},
   "source": [
    "# Importing required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "9889a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textstat import syllable_count\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "fbf0ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a directory called files to store all txt files\n",
    "\n",
    "directory_path = \"C:/Users/Srushti/Documents/blackcoffer\"\n",
    "filesdir = \"files\"\n",
    "\n",
    "directory = os.path.join(directory_path,filesdir)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474855f",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "d6833871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating files for each URL in the dataframe\n",
    "\n",
    "def createfile(url_id,url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status\n",
    "        \n",
    "        file = urllib.request.urlopen(url)\n",
    "        contents = file.read()\n",
    "        htmlfile = contents.decode() #to decode text from bytecode format\n",
    "\n",
    "        soup = BeautifulSoup(htmlfile, 'html.parser')\n",
    "\n",
    "        f = open(os.path.join(directory,str(url_id)+\".txt\"), \"w\")\n",
    "\n",
    "        for data in soup.find_all(\"h1\"):\n",
    "            title = data.get_text()\n",
    "            f.writelines(title)\n",
    "    #         print(title)\n",
    "\n",
    "        class_name = re.compile(r'td-post-content')\n",
    "\n",
    "        for item in soup.find_all(\"div\",class_=class_name):\n",
    "                text = item.get_text()\n",
    "                f.writelines(text)\n",
    "#                 print(text)\n",
    "        f.close()\n",
    "    \n",
    "    #Creating files with \"URL not found\" for cases when a page is not found\n",
    "    except:    \n",
    "        f = open(os.path.join(directory,str(url_id)+\".txt\"), \"w\")\n",
    "        f.write(\"URL not found\")\n",
    "        f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "3f3ed4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing texts under images and footers that were extracted\n",
    "def removefig(filename):\n",
    "    with open(filename,\"r\") as ipfile:\n",
    "        temp = open(\"temp.txt\",\"w\")\n",
    "        for i in ipfile:\n",
    "            if \"Fig:\" in i or \"Blackcoffer Insights\" in i:\n",
    "                continue\n",
    "            else:\n",
    "                temp.write(i)\n",
    "        temp.close()\n",
    "    os.remove(filename)\n",
    "    os.rename(\"temp.txt\",filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d58a8",
   "metadata": {},
   "source": [
    "## Loading Input.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a1d5a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_excel(\"Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f8e0c448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0    37.0  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1    38.0  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2    39.0  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3    40.0  https://insights.blackcoffer.com/will-machine-...\n",
       "4    41.0  https://insights.blackcoffer.com/will-ai-repla..."
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23be59",
   "metadata": {},
   "source": [
    "## Calling the function to createfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5a99a92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,input_df.shape[0]):\n",
    "    createfile(input_df[\"URL_ID\"][i],input_df[\"URL\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb547ab",
   "metadata": {},
   "source": [
    "### Removing the footer and any image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3522d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory,filename)\n",
    "    removefig(file_path)\n",
    "#     print(file_path)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04bd00",
   "metadata": {},
   "source": [
    "# Sentimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd0302",
   "metadata": {},
   "source": [
    "### Removal of Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0c011",
   "metadata": {},
   "source": [
    "#### Creating a list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "5c8675f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "\n",
    "# Iterate over all files in the stop words directory\n",
    "stop_words_dir = os.path.join(directory_path, \"StopWords\")\n",
    "for filename in os.listdir(stop_words_dir):\n",
    "    filepath = os.path.join(stop_words_dir, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "#         # Read the stop words from the file and add them to the set\n",
    "        for line in f:\n",
    "            if \"|\" in line: # to remove additional information about the stopword\n",
    "                newline = line.split('|')[0].strip()\n",
    "#                 print(newline)\n",
    "                stop_words.append(newline.strip())\n",
    "            else:\n",
    "                stop_words.append(line.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "dd7dbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stop words given in the folder from your text in each files\n",
    "def removestopwords(filename):\n",
    "    with open(filename,\"r\") as file:\n",
    "        #Creating a temporary file to remove stopwords\n",
    "            with open(\"temp.txt\",\"w\") as temp:\n",
    "                text = file.read()\n",
    "                words = text.split()\n",
    "                \n",
    "                #remove stop words\n",
    "                filtered_words = [word.lower() for word in words if word.upper() not in stop_words]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                \n",
    "            #     print(len(words))\n",
    "            #     print(len(filtered_words))\n",
    "#                 print(filtered_text)\n",
    "                #Write to the temporary file\n",
    "                temp.write(filtered_text)\n",
    "    os.remove(filename) #delete the file\n",
    "    os.rename(\"temp.txt\",filename)#rename temporary file with filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6741bf8",
   "metadata": {},
   "source": [
    "### Calling the function to clean using stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "868804ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory,filename)\n",
    "#     print(file_path)\n",
    "    removestopwords(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b785a",
   "metadata": {},
   "source": [
    "### Creating dictionary of positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "da5c743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "649a8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "posneg_path = os.path.join(directory_path, \"MasterDictionary\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(posneg_path):\n",
    "    file_path = os.path.join(posneg_path,filename)\n",
    "    with open(file_path,\"r\") as file:\n",
    "        text = file.read()\n",
    "        words = text.split()\n",
    "        #words from \"negative-words.txt\" is added to the dictionary with \n",
    "        #its key value as \"negative\" else add it under the key value \"positive\"\n",
    "        if filename == \"negative-words.txt\":\n",
    "            dictionary[\"negative\"]=words\n",
    "        else:\n",
    "            dictionary[\"positive\"]=words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "158f3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords (if any) from the dictionary\n",
    "\n",
    "for i in dictionary:\n",
    "    for j in dictionary[i]:\n",
    "#         print(j)\n",
    "        if j in stop_words:\n",
    "            dictionary[i].remove(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d548648",
   "metadata": {},
   "source": [
    "#### Creating lists to store values calulated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "affffb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = []\n",
    "total_sent = []\n",
    "complex_words = []\n",
    "words_count = []\n",
    "syllable = []\n",
    "personal_pro = []\n",
    "average_word_length = []\n",
    "words_per_sent = []\n",
    "average_words_per_sentence = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df08a6",
   "metadata": {},
   "source": [
    "### Extracting Derived Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "d6ddaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive=[]\n",
    "negative=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "b599d1ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def posneg_score(filename):\n",
    "    pos=0\n",
    "    neg=0\n",
    "    with open(filename,\"r\") as file:\n",
    "        text = file.read()\n",
    "        tokens = word_tokenize(text)\n",
    "        total_words.append(len(tokens))\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in dictionary[\"positive\"]:\n",
    "                pos+=1\n",
    "            elif token in dictionary[\"negative\"]:\n",
    "                neg+=1\n",
    "#         print(neg)\n",
    "#         print(pos)  \n",
    "        positive.append(pos)\n",
    "        negative.append(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfc6ab",
   "metadata": {},
   "source": [
    "### Counting number of sentence in each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "82908bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(filename):\n",
    "    with open(filename,\"r\") as file:\n",
    "        text = file.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        total_sent.append(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be214d",
   "metadata": {},
   "source": [
    "## Analysis of readbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "3a1fac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check if a word is complex word\n",
    "def is_complex_word(word):\n",
    "    #Use function \"syllable_count\" from nltk library to find the syllables in a word\n",
    "    syllables = syllable_count(word)\n",
    "    if syllables >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Count number of complex words\n",
    "def complex_words_count(filename):\n",
    "    complexw = 0\n",
    "    with open(filename,\"r\") as file:\n",
    "        text = file.read()\n",
    "        #Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        for token in tokens:\n",
    "            if is_complex_word(token):\n",
    "                complexw+=1\n",
    "        complex_words.append(complexw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b4453",
   "metadata": {},
   "source": [
    "## Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "d97225f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate avgerage words per sentence for each file\n",
    "def avg_words_per_sentence(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "        #Toknize sentences using \"sent_tokenize\" fro nltk library\n",
    "        sentences = sent_tokenize(text)\n",
    "        total_words = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            total_words += len(words)\n",
    "        \n",
    "        #Calulating average\n",
    "        avg_words = total_words / len(sentences)\n",
    "        average_words_per_sentence.append(avg_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e7594",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "68d42cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting all the words in a file and storing them in words_count array as\n",
    "def word_count(filename):\n",
    "    with open(filename,\"r\") as file:\n",
    "        with open(\"temp.txt\",\"w\") as temp:\n",
    "            text = file.read()\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            #Removing stopwords using stopwords class of nltk library\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "            words_count.append(len(tokens))\n",
    "            \n",
    "            # join the filtered tokens into a string\n",
    "            filtered_text = ' '.join(filtered_tokens)\n",
    "            temp.write(filtered_text)\n",
    "            \n",
    "    os.remove(filename) #delete the file\n",
    "    os.rename(\"temp.txt\",filename) #remaneing the temporary file with filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb44493",
   "metadata": {},
   "source": [
    "## Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "2c25a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count syllables per word for each file and store in \"syllable\" list\n",
    "def syllables_count(filename):\n",
    "    count_syllables = 0 \n",
    "    \n",
    "    #Here we consider syllables by counting the number of vowels \n",
    "    #unlike the previous time while calulating complex words, \n",
    "    #where we used syllables_count fucntionfrom inbuilt library\n",
    "    \n",
    "    vowels = \"aeiou\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "        # Count syllables for each word\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            #ignoring words that end with \"ed\" or \"es\"\n",
    "            if word[-2:] in ['es', 'ed']:\n",
    "                continue\n",
    "            else:\n",
    "                for letter in word:\n",
    "                    if letter in vowels:\n",
    "                        count_syllables+=1\n",
    "        \n",
    "        #calulating average\n",
    "        avg_syllables = count_syllables/len(words)\n",
    "        syllable.append(avg_syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0524b",
   "metadata": {},
   "source": [
    "## Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "95c82e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count occurences of personal pronouns\n",
    "def personal_pronouns(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "        #use regex to find counts of words with I,we,my,ours,us\n",
    "        counts = {\n",
    "            \"I\": len(re.findall(r\"\\bI\\b\", text)),\n",
    "            \"we\": len(re.findall(r\"\\bwe\\b\", text)),\n",
    "            \"my\": len(re.findall(r\"\\bmy\\b\", text)),\n",
    "            \"ours\": len(re.findall(r\"\\bours\\b\", text)),\n",
    "            \"us\": len(re.findall(r\"\\bus\\b\", text, re.IGNORECASE))\n",
    "        }\n",
    "        \n",
    "        # Exclude US from the counts\n",
    "        if 'US' in text:\n",
    "            counts['us'] -= len(re.findall(r\"\\bUS\\b\", text))\n",
    "        personal_pro.append(sum(counts.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227cb851",
   "metadata": {},
   "source": [
    "## Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "84d10afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calulate average word length for each file\n",
    "def avg_word_length(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "        words = text.split()\n",
    "        word_len = [len(word) for word in words]\n",
    "        \n",
    "        #average\n",
    "        avg_word_len = sum(word_len)/len(words)\n",
    "        average_word_length.append(avg_word_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2daa12e",
   "metadata": {},
   "source": [
    "### Creating a list of file names in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "c4e4f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [str(url)+\".txt\" for url in input_df[\"URL_ID\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d7355",
   "metadata": {},
   "source": [
    "## Calling all the above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8844d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    file_path = os.path.join(directory,filename)\n",
    "    posneg_score(file_path)\n",
    "    count_sentences(file_path)\n",
    "    avg_words_per_sentence(file_path)\n",
    "    word_count(file_path)\n",
    "    complex_words_count(file_path)\n",
    "    avg_word_length(file_path)\n",
    "    syllables_count(file_path)\n",
    "    personal_pronouns(file_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6d74a",
   "metadata": {},
   "source": [
    "## Calculating polarity, subjectivity, average sentence length, complex percent, fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "27dc7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = []\n",
    "subjectivity = []\n",
    "average_sentence_length = []\n",
    "complex_percent = []\n",
    "fog_index = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "6fae6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(positive)):\n",
    "    res = (positive[i]-negative[i])/((positive[i]+negative[i])+0.000001)\n",
    "    polarity.append(res)\n",
    "    \n",
    "    subject = (positive[i]+negative[i])/(total_words[i]+0.000001)\n",
    "    subjectivity.append(subject)\n",
    "    \n",
    "    avglen = total_words[i]/total_sent[i]\n",
    "    average_sentence_length.append(avglen)\n",
    "    \n",
    "    comp = complex_words[i]/total_words[i]\n",
    "    complex_percent.append(comp)\n",
    "    \n",
    "    fogin = 0.4*(avglen + comp)\n",
    "    fog_index.append(fogin)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c8fc6",
   "metadata": {},
   "source": [
    "## Adding all values to the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ffccfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"POSITIVE SCORE\"] = positive\n",
    "input_df[\"NEGATIVE SCORE\"] = negative\n",
    "input_df[\"POLARITY SCORE\"] = polarity\n",
    "input_df[\"SUBJECTIVITY SCORE\"] = subjectivity\n",
    "input_df[\"AVG SENTENCE LENGTH\"] = average_sentence_length\n",
    "input_df[\"PERCENTAGE OF COMPLEX WORDS\"] = complex_percent\n",
    "input_df[\"FOG INDEX\"] = fog_index\n",
    "input_df[\"AVG NUMBER OF WORDS PER SENTENCE\"] = average_words_per_sentence\n",
    "input_df[\"COMPLEX WORD COUNT\"] = complex_words\n",
    "input_df[\"WORD COUNT\"] = words_count\n",
    "input_df[\"SYLLABLE PER WORD\"] = syllable\n",
    "input_df[\"PERSONAL PRONOUNS\"] = personal_pro\n",
    "input_df[\"AVG WORD LENGTH\"] = average_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "89003f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.072508</td>\n",
       "      <td>17.421053</td>\n",
       "      <td>0.602719</td>\n",
       "      <td>7.209509</td>\n",
       "      <td>17.421053</td>\n",
       "      <td>798</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.375345</td>\n",
       "      <td>1</td>\n",
       "      <td>7.333027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>60</td>\n",
       "      <td>38</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.099695</td>\n",
       "      <td>12.443038</td>\n",
       "      <td>0.422177</td>\n",
       "      <td>5.146086</td>\n",
       "      <td>12.443038</td>\n",
       "      <td>415</td>\n",
       "      <td>794</td>\n",
       "      <td>2.042313</td>\n",
       "      <td>3</td>\n",
       "      <td>6.593794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>65</td>\n",
       "      <td>37</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>14.117647</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>5.885059</td>\n",
       "      <td>14.117647</td>\n",
       "      <td>714</td>\n",
       "      <td>1007</td>\n",
       "      <td>2.348517</td>\n",
       "      <td>2</td>\n",
       "      <td>7.353814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>68</td>\n",
       "      <td>28</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>10.863158</td>\n",
       "      <td>0.506783</td>\n",
       "      <td>4.547976</td>\n",
       "      <td>10.863158</td>\n",
       "      <td>523</td>\n",
       "      <td>867</td>\n",
       "      <td>2.295792</td>\n",
       "      <td>3</td>\n",
       "      <td>6.621287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>15.556962</td>\n",
       "      <td>0.510171</td>\n",
       "      <td>6.426853</td>\n",
       "      <td>15.556962</td>\n",
       "      <td>627</td>\n",
       "      <td>1045</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>6.753715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>https://insights.blackcoffer.com/man-and-machi...</td>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.074527</td>\n",
       "      <td>14.983333</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>6.186882</td>\n",
       "      <td>14.983333</td>\n",
       "      <td>435</td>\n",
       "      <td>746</td>\n",
       "      <td>2.170623</td>\n",
       "      <td>3</td>\n",
       "      <td>6.734421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43.0</td>\n",
       "      <td>https://insights.blackcoffer.com/in-future-or-...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.064579</td>\n",
       "      <td>11.355556</td>\n",
       "      <td>0.540117</td>\n",
       "      <td>4.758269</td>\n",
       "      <td>11.355556</td>\n",
       "      <td>276</td>\n",
       "      <td>411</td>\n",
       "      <td>2.023499</td>\n",
       "      <td>2</td>\n",
       "      <td>6.874674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-neural-ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-machine-l...</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.100410</td>\n",
       "      <td>13.942857</td>\n",
       "      <td>0.485656</td>\n",
       "      <td>5.771405</td>\n",
       "      <td>13.942857</td>\n",
       "      <td>237</td>\n",
       "      <td>422</td>\n",
       "      <td>1.984169</td>\n",
       "      <td>0</td>\n",
       "      <td>6.279683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46.0</td>\n",
       "      <td>https://insights.blackcoffer.com/deep-learning...</td>\n",
       "      <td>66</td>\n",
       "      <td>35</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.071835</td>\n",
       "      <td>17.575000</td>\n",
       "      <td>0.543385</td>\n",
       "      <td>7.247354</td>\n",
       "      <td>17.575000</td>\n",
       "      <td>764</td>\n",
       "      <td>1207</td>\n",
       "      <td>2.296496</td>\n",
       "      <td>1</td>\n",
       "      <td>6.791554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-to-protec...</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "      <td>-0.119266</td>\n",
       "      <td>0.067744</td>\n",
       "      <td>15.323810</td>\n",
       "      <td>0.471100</td>\n",
       "      <td>6.317964</td>\n",
       "      <td>15.323810</td>\n",
       "      <td>758</td>\n",
       "      <td>1317</td>\n",
       "      <td>2.075342</td>\n",
       "      <td>2</td>\n",
       "      <td>6.416952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-machines-...</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.067651</td>\n",
       "      <td>14.017241</td>\n",
       "      <td>0.608856</td>\n",
       "      <td>5.850439</td>\n",
       "      <td>14.017241</td>\n",
       "      <td>495</td>\n",
       "      <td>706</td>\n",
       "      <td>2.546547</td>\n",
       "      <td>3</td>\n",
       "      <td>7.447447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-human-robo...</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.065646</td>\n",
       "      <td>11.870130</td>\n",
       "      <td>0.473742</td>\n",
       "      <td>4.937549</td>\n",
       "      <td>11.870130</td>\n",
       "      <td>433</td>\n",
       "      <td>763</td>\n",
       "      <td>2.120226</td>\n",
       "      <td>2</td>\n",
       "      <td>6.437058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-ai-will-c...</td>\n",
       "      <td>62</td>\n",
       "      <td>24</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.086519</td>\n",
       "      <td>15.777778</td>\n",
       "      <td>0.545272</td>\n",
       "      <td>6.529220</td>\n",
       "      <td>15.777778</td>\n",
       "      <td>542</td>\n",
       "      <td>880</td>\n",
       "      <td>2.197802</td>\n",
       "      <td>10</td>\n",
       "      <td>6.838828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>51.0</td>\n",
       "      <td>https://insights.blackcoffer.com/future-of-wor...</td>\n",
       "      <td>69</td>\n",
       "      <td>22</td>\n",
       "      <td>0.516484</td>\n",
       "      <td>0.080106</td>\n",
       "      <td>14.379747</td>\n",
       "      <td>0.514085</td>\n",
       "      <td>5.957533</td>\n",
       "      <td>14.379747</td>\n",
       "      <td>584</td>\n",
       "      <td>957</td>\n",
       "      <td>2.212815</td>\n",
       "      <td>1</td>\n",
       "      <td>6.855835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>52.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-tool-alexa...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070225</td>\n",
       "      <td>16.181818</td>\n",
       "      <td>0.567416</td>\n",
       "      <td>6.699694</td>\n",
       "      <td>16.181818</td>\n",
       "      <td>202</td>\n",
       "      <td>298</td>\n",
       "      <td>2.535971</td>\n",
       "      <td>0</td>\n",
       "      <td>7.517986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>53.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-healthcare...</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.117108</td>\n",
       "      <td>109.111111</td>\n",
       "      <td>0.540733</td>\n",
       "      <td>43.860738</td>\n",
       "      <td>109.111111</td>\n",
       "      <td>531</td>\n",
       "      <td>951</td>\n",
       "      <td>2.221321</td>\n",
       "      <td>2</td>\n",
       "      <td>6.577057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54.0</td>\n",
       "      <td>https://insights.blackcoffer.com/all-you-need-...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>12.107143</td>\n",
       "      <td>0.554572</td>\n",
       "      <td>5.064686</td>\n",
       "      <td>12.107143</td>\n",
       "      <td>376</td>\n",
       "      <td>568</td>\n",
       "      <td>2.332061</td>\n",
       "      <td>0</td>\n",
       "      <td>6.965649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>55.0</td>\n",
       "      <td>https://insights.blackcoffer.com/evolution-of-...</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.036832</td>\n",
       "      <td>13.575000</td>\n",
       "      <td>0.556169</td>\n",
       "      <td>5.652468</td>\n",
       "      <td>13.575000</td>\n",
       "      <td>302</td>\n",
       "      <td>448</td>\n",
       "      <td>2.351220</td>\n",
       "      <td>0</td>\n",
       "      <td>7.570732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>56.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-data-anal...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.031532</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>7.648649</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>138</td>\n",
       "      <td>188</td>\n",
       "      <td>2.157303</td>\n",
       "      <td>0</td>\n",
       "      <td>7.123596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0     37.0  https://insights.blackcoffer.com/ai-in-healthc...              64   \n",
       "1     38.0  https://insights.blackcoffer.com/what-if-the-c...              60   \n",
       "2     39.0  https://insights.blackcoffer.com/what-jobs-wil...              65   \n",
       "3     40.0  https://insights.blackcoffer.com/will-machine-...              68   \n",
       "4     41.0  https://insights.blackcoffer.com/will-ai-repla...              58   \n",
       "5     42.0  https://insights.blackcoffer.com/man-and-machi...              45   \n",
       "6     43.0  https://insights.blackcoffer.com/in-future-or-...              22   \n",
       "7     44.0  https://insights.blackcoffer.com/how-neural-ne...               0   \n",
       "8     45.0  https://insights.blackcoffer.com/how-machine-l...              36   \n",
       "9     46.0  https://insights.blackcoffer.com/deep-learning...              66   \n",
       "10    47.0  https://insights.blackcoffer.com/how-to-protec...              48   \n",
       "11    48.0  https://insights.blackcoffer.com/how-machines-...              33   \n",
       "12    49.0  https://insights.blackcoffer.com/ai-human-robo...              31   \n",
       "13    50.0  https://insights.blackcoffer.com/how-ai-will-c...              62   \n",
       "14    51.0  https://insights.blackcoffer.com/future-of-wor...              69   \n",
       "15    52.0  https://insights.blackcoffer.com/ai-tool-alexa...              25   \n",
       "16    53.0  https://insights.blackcoffer.com/ai-healthcare...              76   \n",
       "17    54.0  https://insights.blackcoffer.com/all-you-need-...              24   \n",
       "18    55.0  https://insights.blackcoffer.com/evolution-of-...              15   \n",
       "19    56.0  https://insights.blackcoffer.com/how-data-anal...               3   \n",
       "\n",
       "    NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0               32        0.333333            0.072508            17.421053   \n",
       "1               38        0.224490            0.099695            12.443038   \n",
       "2               37        0.274510            0.085000            14.117647   \n",
       "3               28        0.416667            0.093023            10.863158   \n",
       "4               25        0.397590            0.067535            15.556962   \n",
       "5               22        0.343284            0.074527            14.983333   \n",
       "6               11        0.333333            0.064579            11.355556   \n",
       "7                0        0.000000            0.000000             2.000000   \n",
       "8               13        0.469388            0.100410            13.942857   \n",
       "9               35        0.306931            0.071835            17.575000   \n",
       "10              61       -0.119266            0.067744            15.323810   \n",
       "11              22        0.200000            0.067651            14.017241   \n",
       "12              29        0.033333            0.065646            11.870130   \n",
       "13              24        0.441860            0.086519            15.777778   \n",
       "14              22        0.516484            0.080106            14.379747   \n",
       "15               0        1.000000            0.070225            16.181818   \n",
       "16              39        0.321739            0.117108           109.111111   \n",
       "17               0        1.000000            0.035398            12.107143   \n",
       "18               5        0.500000            0.036832            13.575000   \n",
       "19               4       -0.142857            0.031532            18.500000   \n",
       "\n",
       "    PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                      0.602719   7.209509                         17.421053   \n",
       "1                      0.422177   5.146086                         12.443038   \n",
       "2                      0.595000   5.885059                         14.117647   \n",
       "3                      0.506783   4.547976                         10.863158   \n",
       "4                      0.510171   6.426853                         15.556962   \n",
       "5                      0.483871   6.186882                         14.983333   \n",
       "6                      0.540117   4.758269                         11.355556   \n",
       "7                      0.000000   0.800000                          2.000000   \n",
       "8                      0.485656   5.771405                         13.942857   \n",
       "9                      0.543385   7.247354                         17.575000   \n",
       "10                     0.471100   6.317964                         15.323810   \n",
       "11                     0.608856   5.850439                         14.017241   \n",
       "12                     0.473742   4.937549                         11.870130   \n",
       "13                     0.545272   6.529220                         15.777778   \n",
       "14                     0.514085   5.957533                         14.379747   \n",
       "15                     0.567416   6.699694                         16.181818   \n",
       "16                     0.540733  43.860738                        109.111111   \n",
       "17                     0.554572   5.064686                         12.107143   \n",
       "18                     0.556169   5.652468                         13.575000   \n",
       "19                     0.621622   7.648649                         18.500000   \n",
       "\n",
       "    COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                  798        1129           2.375345                  1   \n",
       "1                  415         794           2.042313                  3   \n",
       "2                  714        1007           2.348517                  2   \n",
       "3                  523         867           2.295792                  3   \n",
       "4                  627        1045           2.166667                  8   \n",
       "5                  435         746           2.170623                  3   \n",
       "6                  276         411           2.023499                  2   \n",
       "7                    0           2           1.500000                  0   \n",
       "8                  237         422           1.984169                  0   \n",
       "9                  764        1207           2.296496                  1   \n",
       "10                 758        1317           2.075342                  2   \n",
       "11                 495         706           2.546547                  3   \n",
       "12                 433         763           2.120226                  2   \n",
       "13                 542         880           2.197802                 10   \n",
       "14                 584         957           2.212815                  1   \n",
       "15                 202         298           2.535971                  0   \n",
       "16                 531         951           2.221321                  2   \n",
       "17                 376         568           2.332061                  0   \n",
       "18                 302         448           2.351220                  0   \n",
       "19                 138         188           2.157303                  0   \n",
       "\n",
       "    AVG WORD LENGTH  \n",
       "0          7.333027  \n",
       "1          6.593794  \n",
       "2          7.353814  \n",
       "3          6.621287  \n",
       "4          6.753715  \n",
       "5          6.734421  \n",
       "6          6.874674  \n",
       "7          4.000000  \n",
       "8          6.279683  \n",
       "9          6.791554  \n",
       "10         6.416952  \n",
       "11         7.447447  \n",
       "12         6.437058  \n",
       "13         6.838828  \n",
       "14         6.855835  \n",
       "15         7.517986  \n",
       "16         6.577057  \n",
       "17         6.965649  \n",
       "18         7.570732  \n",
       "19         7.123596  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eda158",
   "metadata": {},
   "source": [
    "## Storing the dataframe in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "d503465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_csv(\"Output.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
